# Default values for sombra-chart.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

#  Namespace in which resources will be deployed
enabled: false
namespace: transcend
#  Number of instances of Datadog Operator
replicaCount: 1
# Set it to `true` to deploy llm-classifier resources. 
enabled: false

image:
  # Repository to use for llm-classifier image
  repository: docker.transcend.io/llm-classifier
  # Define the pullPolicy for llm-classifier image
  pullPolicy: IfNotPresent
  # llm-classifer version to deploy
  tag: "v1.0.0"

# Override name of app
nameOverride: ""
# Override the full qualified app name
fullnameOverride: ""

# Define annotations for sombra pod
podAnnotations: {}
# Define lables for sombra pod
podLabels: {}

# Define llm-classifier networking
service:
  type: ClusterIP
  port: 6081

envs:
  - name: LLM_SERVER_PORT
    value: '6081'
  - name: LLM_SERVER_CONCURRENCY
    value: '2'
  - name: LLM_SERVER_TIMEOUT
    value: '120'

# Define resources as per required throughput
# Make sure cluster support `nvidia.com/gpu` 
resources:
  limits:
    memory: 8Gi
    nvidia.com/gpu: '1'

livenessProbe:
  httpGet:
    path: /health/ping
    port: 6081
    scheme: HTTP
  timeoutSeconds: 30
  periodSeconds: 10
  successThreshold: 1
  failureThreshold: 10

startupProbe:
  httpGet:
    path: /health/ping
    port: 6081
    scheme: HTTP
  timeoutSeconds: 30
  periodSeconds: 20
  successThreshold: 1
  failureThreshold: 10

# Enable to use horizontal pod autoscaling based of metrics
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 5
  targetCPUUtilizationPercentage: 80
  targetMemoryUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity: {}
